# **Setting Up a Standalone Spark Cluster on Windows** ğŸš€  

### *DataTalksClub - Data Engineering Zoomcamp 2025*

<br>

> **A Step-by-Step Guide for Data Engineers** | DataTalksClub Data Engineering Zoomcamp 2025  




<br>

## **ğŸ“Œ Overview**
This guide walks through setting up a **standalone Apache Spark cluster on Windows**, allowing you to run Spark applications **locally with a master and worker nodes**.

### **Table of Contents**
âœ… [Prerequisites](#prerequisites)  
âœ… [Starting the Spark Master](#starting-the-spark-master)  
âœ… [Starting a Worker Node](#starting-a-worker-node)  
âœ… [Connecting an Application to the Cluster](#connecting-an-application-to-the-cluster)  
âœ… [Best Practices](#best-practices)  

<br>

## **1ï¸âƒ£ Prerequisites**
Before setting up Spark, make sure you have:
âœ… **Java 8+ Installed**: Spark requires Java Development Kit (JDK)  
âœ… **Apache Spark Installed**: Download from [Apache Spark Website](https://spark.apache.org/downloads.html)  
âœ… **SPARK_HOME Set Up**: Environment variable pointing to your Spark installation folder  
âœ… **Python Installed**: Needed plan to run PySpark  

### **Verify Java Installation**
Run in Command Prompt (CMD):
```sh
java -version
```


### **Verify Spark Installation**
Make sure you have this file:
```sh
%SPARK_HOME%\bin\spark-shell
```
If Spark starts successfully, your setup is correct.

<br>

## **2ï¸âƒ£ Starting the Spark Master**
To start the **Spark Master node**, follow these steps:

1ï¸âƒ£ **Open CMD** and navigate to the Spark `bin` directory:
```sh
cd %SPARK_HOME%\bin
```

2ï¸âƒ£ **Run the Spark Master process**:
```sh
spark-class org.apache.spark.deploy.master.Master
```
ğŸ”¹ The **Master URL** will be displayed in the format:  
   ```
   spark://your-ip:7077
   ```
   (Make a note of this for later.)

<br>

## **3ï¸âƒ£ Starting a Worker Node**
To start a **worker node**, follow these steps:

1ï¸âƒ£ Open a **new CMD window** and navigate to the Spark `bin` directory:
```sh
cd %SPARK_HOME%\bin
```

2ï¸âƒ£ Run the worker node, replacing `<MASTER_URL>` with the **Spark Master URL** from the previous step:
```sh
spark-class org.apache.spark.deploy.worker.Worker <MASTER_URL>
```
Example:
```sh
spark-class org.apache.spark.deploy.worker.Worker spark://192.168.1.10:7077
```
âœ” You should now see a **worker connected** to the master.

<br>

## **4ï¸âƒ£ Connecting an Application to the Cluster**
Now that the cluster is running, you can connect a **Spark application**.

1ï¸âƒ£ Start the **Spark Shell** with the master node:
```sh
spark-shell --master spark://your-ip:7077
```

2ï¸âƒ£ Run a sample Spark job:
```scala
val data = spark.range(1, 1000000)
data.count()
```
âœ” This confirms that Spark is running correctly!

<br>

## **5ï¸âƒ£ Best Practices**
ğŸ’¡ **Ensure Java & Spark Paths Are Set Correctly** â€“ Add them to system environment variables  
ğŸ’¡ **Allocate Enough Memory for Workers** â€“ Use `spark.executor.memory` for tuning  
ğŸ’¡ **Monitor Spark Web UI** â€“ Open `http://localhost:8080` to see master and workers  
ğŸ’¡ **Use Parquet or ORC for Data Storage** â€“ Optimized for Spark processing  
