# **Batch Processing with Apache Spark**  

### *DataTalksClub - Data Engineering Zoomcamp 2025*


<br>

## **ğŸ“Œ Overview**

This repository contains my **notes and insights** from **Module 5 (Batch Processing - Spark)** of the **DataTalksClub Data Engineering Zoomcamp 2025**. In this module, I explored **Apache Spark**, focusing on **batch processing, Spark SQL, DataFrames, and cluster internals**.

âœ… [Introduction to Batch Processing](#introduction-to-batch-processing)  
âœ… [Introduction to Apache Spark](#introduction-to-apache-spark)  
âœ… [Spark SQL and DataFrames](#spark-sql-and-dataframes)  
âœ… [Spark Internals (Cluster and GroupBy)](#spark-internals-cluster-and-groupby)  
âœ… [Best Practices for Optimizing Spark Workloads](#best-practices-for-optimizing-spark-workloads)  

<br>

## **ğŸ“Œ Introduction to Batch Processing**
### **What is Batch Processing?**
Batch processing is a **data processing method** where large volumes of data are collected, stored, and processed in **chunks (batches)** instead of being processed in real-time.

### **Key Characteristics of Batch Processing:**
âœ” **Efficient for Large Datasets** â€“ Suitable for ETL workflows and analytical workloads.  
âœ” **Scalability** â€“ Can process **terabytes to petabytes** of data using distributed computing.  
âœ” **Optimized for Throughput** â€“ Trades **latency for performance** to process data in bulk.  

### **Common Use Cases**
1ï¸âƒ£ **ETL Pipelines** â€“ Extracting, transforming, and loading data into a Data Warehouse.  
2ï¸âƒ£ **Data Lake Processing** â€“ Transforming raw S3/HDFS files into structured datasets.  
3ï¸âƒ£ **Machine Learning Model Training** â€“ Processing historical data for training ML models.  

<br>

## **ğŸ”¥ Introduction to Apache Spark**
### **What is Apache Spark?**
Apache Spark is an **open-source distributed computing engine** optimized for **big data processing and analytics**. It supports **batch processing, streaming, SQL, machine learning, and graph processing**.

### **Key Features**
ğŸ”¹ **In-Memory Computation** â€“ Faster processing compared to Hadoop.  
ğŸ”¹ **Scalability** â€“ Runs on **clusters with thousands of nodes**.  
ğŸ”¹ **Unified API** â€“ Supports **SQL, Python (PySpark), Java, Scala, and R**.  
ğŸ”¹ **Supports Data Lakes** â€“ Works with **AWS S3, HDFS, Delta Lake, and Parquet**.  

### **How Spark Works**
1. **Driver Program** â€“ Sends execution requests to Spark.  
2. **Cluster Manager** â€“ Distributes work across **Executor Nodes**.  
3. **Executors** â€“ Perform the actual computation in parallel.  
4. **Resilient Distributed Dataset (RDD)** â€“ Sparkâ€™s internal data structure for distributed computing.  

<br>

## **ğŸ“Œ Spark SQL and DataFrames**
### **What is Spark SQL?**
Spark SQL allows you to **query structured data using SQL syntax** while benefiting from Sparkâ€™s distributed engine.

### **DataFrames vs RDDs**
| Feature | DataFrame | RDD |
|---------|----------|-----|
| Performance | **Optimized** | Slower |
| API | SQL-like | Low-level |
| Schema | **Enforced** | Not enforced |
| Use Case | **Analytics, ETL, Reporting** | Complex transformations |

### **ğŸ› ï¸ Creating a Spark DataFrame**
```python
from pyspark.sql import SparkSession

# Initialize Spark Session
spark = SparkSession.builder \
    .master("local[*]") \
    .appName('test') \
    .getOrCreate()

# Read data from a Parquet file
df = spark.read.parquet("/local_sotage/nyc_taxi_data/")
df.show()
```

### **ğŸ› ï¸ Querying Data Using Spark SQL**
```python
df.createOrReplaceTempView("nyc_taxi")

result = spark.sql("""
SELECT vendor_id, COUNT(*) as trip_count
FROM nyc_taxi
GROUP BY vendor_id
""")

result.show()
```

<br>

## **ğŸ“Œ Spark Internals**
### **Spark Cluster Architecture**
Spark operates in a **distributed computing environment** with the following components:

1. **Driver Program** â€“ The main entry point for Spark applications.  
2. **Cluster Manager** â€“ Allocates resources (e.g., YARN, Kubernetes, Mesos, Standalone).  
3. **Executors** â€“ Worker nodes that **process tasks in parallel**.  
4. **Tasks** â€“ Individual computation units executed by Executors.  

### **ğŸ› ï¸ Understanding GroupBy in Spark**
- Spark **shuffles data** across nodes during `groupBy()`, which can be **expensive**.  
- **Alternative**: Use `groupByKey()` sparingly or **reduce data movement** using partitions.  

#### **Example of GroupBy Aggregation**
```python
from pyspark.sql.functions import col, count

df.groupBy("vendor_id").agg(count("*").alias("trip_count")).show()
```

#### **Optimized Approach with Window Functions**
```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

window_spec = Window.partitionBy("vendor_id").orderBy(col("trip_distance").desc())

df.withColumn("rank", row_number().over(window_spec)).show()
```

<br>

## **âœ… Best Practices for Optimizing Spark Workloads**
ğŸš€ **Partitioning & Bucketing** â€“ Store large datasets in partitioned tables to **reduce scan time**.  
ğŸš€ **Cache & Persist** â€“ Use `.cache()` or `.persist()` for **frequent re-use of datasets**.  
ğŸš€ **Optimize File Formats** â€“ Use **Parquet/ORC** instead of CSV for **faster processing**.  
ğŸš€ **Broadcast Joins** â€“ For small tables, use `broadcast(df)` to **avoid expensive shuffles**.  
ğŸš€ **Reduce Shuffle Operations** â€“ Minimize **`groupBy()` and `.join()`** operations to improve efficiency.  

<br>


## **ğŸ“š Additional Resources**
ğŸ”— [Apache Spark Docs](https://spark.apache.org/docs/latest/)  
ğŸ”— [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)   
